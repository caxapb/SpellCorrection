{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Norvig's solution\n",
        "\n",
        "I checked the Norvig's solution and got several points from there (it's actually a great point for the beginning). To see how good it is I copied the code for the further testing and the Norvig's dataset 'big.txt'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def get_words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "file = open('useful data/big.txt').read()\n",
        "WORDS = Counter(get_words(file))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]).union(known(edits1(word))) or known(edits2(word)) or [word])\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# form the dictionaries: bigrams, threegrams, fourgrams, fivegrams, and pos_grams\n",
        "# each dictionary has keys with possible n_grams\n",
        "\n",
        "bigrams = {}\n",
        "threegrams = {}\n",
        "fourgrams = {}\n",
        "fivegrams = {}\n",
        "pos_bigrams = {}\n",
        "\n",
        "# generate 3-4-5 grams from the fivegrams.txt\n",
        "def generate_n_grams(line: str):\n",
        "    parts = line.split()\n",
        "    freq = int(parts[0])\n",
        "    fivegram = parts[1:]\n",
        "    for i in range(3):\n",
        "        three_words = (fivegram[i], fivegram[i+1], fivegram[i+2])\n",
        "        if three_words in threegrams.keys():\n",
        "            threegrams[three_words] += freq\n",
        "        else:\n",
        "            threegrams[three_words] = freq\n",
        "    for i in range(2):\n",
        "        four_words = (fivegram[i], fivegram[i+1], fivegram[i+2], fivegram[i+3])\n",
        "        if four_words in fourgrams.keys():\n",
        "            fourgrams[four_words] += freq\n",
        "        else:\n",
        "            fourgrams[four_words] = freq\n",
        "    fivegram = tuple(fivegram)\n",
        "    if fivegram in fivegrams.keys():\n",
        "        fivegrams[(fivegram)] += freq\n",
        "    else:\n",
        "        fivegrams[(fivegram)] = freq\n",
        "\n",
        "# generate bigrams fromt he bigrams.txt\n",
        "def generate_2_grams(line: str):\n",
        "    parts = line.split()\n",
        "    two_words = (parts[1], parts[2])\n",
        "    if two_words in bigrams.keys():\n",
        "        bigrams[two_words] += int(parts[0])\n",
        "    else:\n",
        "        bigrams[two_words] = int(parts[0])\n",
        "\n",
        "# apply functions\n",
        "filename = 'useful data/fivegrams.txt'\n",
        "with open(filename, 'r', encoding='latin-1') as f:\n",
        "    for line in f:\n",
        "        generate_n_grams(line)\n",
        "filename = 'useful data/bigrams.txt'\n",
        "with open(filename, 'r', encoding='latin-1') as f:\n",
        "    for line in f:\n",
        "        generate_2_grams(line)\n",
        "\n",
        "# also extract bigrams woth pos tags from the coca_all_links.txt\n",
        "with open('useful data/coca_all_links.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        parts = line.split()\n",
        "        freq, w1, w2, pos1, pos2 = parts\n",
        "        tup = (w1, pos1, w2, pos2)\n",
        "        if tup in pos_bigrams.keys():\n",
        "            pos_bigrams[tup] += int(freq)\n",
        "        else:\n",
        "            pos_bigrams[tup] = int(freq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the class with my spell corrector\n",
        "import spacy\n",
        "\n",
        "class Corrector():\n",
        "    def __init__(self,):\n",
        "        \"\"\" Init method. Calls the method init_vars to initialize variables \"\"\"\n",
        "        self.init_vars()\n",
        "\n",
        "    def init_vars(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.log = []\n",
        "        self.result = []\n",
        "        self.words = None\n",
        "        self.tagged = None\n",
        "\n",
        "    def correct_text(self, text: str, return_list: bool=True) -> list[str]:\n",
        "        \"\"\" Method for correction of the whole text with several sentences\n",
        "        Args:\n",
        "            text (str): text splitted on sentences by '.'\n",
        "        Returns:\n",
        "            result (list or str): corrected text. The return type depends on\n",
        "            return_list variable. If True => return list of corrected sentences.\n",
        "            If False => return the text in the str format.\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        text = text.split('. ')\n",
        "        for sentence in text:\n",
        "            # split the text into sentences and correct them one by one\n",
        "            result.append(self.correct_sentence(sentence))\n",
        "\n",
        "        if return_list:\n",
        "            return result\n",
        "        return '. '.join(result)\n",
        "    \n",
        "\n",
        "    def correct_sentence(self, sentence: str) -> str:\n",
        "        \"\"\" Correction of a passed sentence.\n",
        "        Args:\n",
        "            sentence (str): sentence with misspelled words\n",
        "        Returns:\n",
        "            self.result (str): corrected sentence\n",
        "        \"\"\"\n",
        "        \n",
        "        # split the sentence to words and tag them to determine expected tags\n",
        "        self.words = get_words(sentence)\n",
        "        self.tagged = self.nlp(sentence)\n",
        "        self.result = []\n",
        "\n",
        "        for i, word in enumerate(self.words):\n",
        "            # check the correctness of each word\n",
        "\n",
        "            if word in WORDS:\n",
        "                # if the current word exists in the vocabulary\n",
        "                # add it to the result and skip\n",
        "                self.result.append(word)\n",
        "                continue\n",
        "            \n",
        "            # identify the context of the current word\n",
        "            # (all previous tokens)\n",
        "            context = self.words[:i]\n",
        "\n",
        "            # if it is teh 1st word, then context is nothing\n",
        "            # apply candidates generation and check the pos tags compatibility\n",
        "            if not context:\n",
        "                self.check_1st_word(word)\n",
        "                continue\n",
        "\n",
        "            # otherwise word is incorrect and isn't the 1st one\n",
        "            expected_tag = self.tagged[i].dep_\n",
        "\n",
        "            # get available variants and sort them by frequency in the corpus\n",
        "            variants = self.get_variants(word)\n",
        "            variants = sorted(list(variants), key=P, reverse=True)\n",
        "            \n",
        "            # if there is no available candidates we cannot choose and\n",
        "            # paste the same incorrect word\n",
        "            if not variants:\n",
        "                self.result.append(word)\n",
        "                continue\n",
        "\n",
        "            \n",
        "            # 1. for each variant we compute some statistics: in how many n_grams types the current context+variant is met \n",
        "            # and how many times it is met (using n_grams dictionaries)\n",
        "            # variables: types_of_grams and occurences\n",
        "            # 2.1. if the current context+variant cannot be found in any n_grams\n",
        "            # (types_of_grams = 0), we ignore it for now\n",
        "            # 2.2. if the current context+variant is found, check if it is the best suitable variant for this context:\n",
        "            # if types_of_grams > max_gram: save max_gram, max_occurences and the index of this variant\n",
        "            # if types_of_grams == max_gram: check if current variant is met more times than the one found so far,\n",
        "            # if so, reassign max_occurences and index variables\n",
        "            # 3.1 if max_gram != 0: if at least 1 context+variant is met in the corpus, then choose the one variants[index]\n",
        "            # (with the biggest types_of_grams and occurences)\n",
        "            # 3.2 otherwise, context+variable doesn't exist in the corpus and correct it with usual Norwig's correction()\n",
        "\n",
        "            max_gram = 0\n",
        "            max_occurences = 0\n",
        "            index = 0\n",
        "\n",
        "            # iterate over all variants\n",
        "            for j, variant in enumerate(variants):\n",
        "                # concatenate the context (previous words) and the current variant\n",
        "                variant_sentence = context+[variant]\n",
        "\n",
        "                # see the self.find_grams_freq method description bellow ->\n",
        "                types_of_grams, occurences = self.find_grams_freq(variant_sentence)\n",
        "                \n",
        "\n",
        "                if types_of_grams == 0:\n",
        "                    # this context+variant isn't met in the corpus\n",
        "                    continue\n",
        "                \n",
        "                if types_of_grams > max_gram:\n",
        "                    max_gram = types_of_grams\n",
        "                    max_occurences = occurences\n",
        "                    index = j\n",
        "                if types_of_grams == max_gram:\n",
        "                    if max_occurences < occurences:\n",
        "                        max_occurences = occurences\n",
        "                        index = j\n",
        "\n",
        "            if max_gram != 0:\n",
        "                correct_word = variants[index]\n",
        "            else:\n",
        "                correct_word = correction(word)\n",
        "\n",
        "            self.result.append(correct_word)\n",
        "        self.result = ' '.join(self.result)\n",
        "        return self.result\n",
        "\n",
        "\n",
        "    def check_1st_word(self, word: str):\n",
        "        \"\"\" Method for selection of the correct 1st word, if it is misspelled\n",
        "        Args:\n",
        "            word (str): the 1st word in the sentence\n",
        "        Returns: None\n",
        "            depending on the analysis, the method correct the 1st word\n",
        "            and appends it to the result\n",
        "        \"\"\"\n",
        "        # get available variants for this word and sort them by frequency\n",
        "        variants = self.get_variants(word)\n",
        "        variants = sorted(list(variants), key=P, reverse=True)\n",
        "        # if there is no available variants return the same word\n",
        "        if len(variants) == 0: \n",
        "            self.result.append(word)\n",
        "            return\n",
        "\n",
        "        # iteratively check each of them\n",
        "        for variant in variants:\n",
        "            # select the correct word based on expected tag\n",
        "            # use the tagged current sentence from which the word is taken\n",
        "            # and form a new \"corrected\" sentence with the current variant\n",
        "            expected_tag = self.tagged[0].dep_\n",
        "            variant_sentence = variant + ' '.join(self.words[1:])\n",
        "\n",
        "            # if the current variant suits, based on expected POS tag, then ...\n",
        "            ok = self.check_tag_accordance(variant_sentence, expected_tag, 0)\n",
        "\n",
        "            if ok:\n",
        "                # ... select this one\n",
        "                # this method allows to combine Norvig's correction\n",
        "                # and filter unsuitable variants via POS tags\n",
        "                self.result.append(variant)\n",
        "                return\n",
        "            \n",
        "        # if we have some variants but none of them is assigned as an expected tag,\n",
        "        # return variants[0] - the same as a basic Norvig's correction()\n",
        "        self.result.append(variants[0])\n",
        "\n",
        "\n",
        "    def check_tag_accordance(self, sentence: str, expected_tag, i: int) -> bool:\n",
        "        \"\"\" Method for checking do an expected tag and variant's tag coincide\n",
        "        Args:\n",
        "            sentence (str): sentence with inserted variant word instead of misspelled one\n",
        "            expected_tag (Token): tag of a misspelled, gotten from the initial sentence (with a mistake)\n",
        "            i (int): index of a misspelled word\n",
        "        Returns:\n",
        "            bool: coincide or not\n",
        "        \"\"\"\n",
        "        tokens = self.nlp(sentence)\n",
        "        return tokens[i].dep_ == expected_tag\n",
        "\n",
        "\n",
        "    def get_variants(self, word: str) -> list[str]:\n",
        "        \"\"\" Method for generating available candidates as a correct word instead of a misspelled word\n",
        "        Args:\n",
        "            word (str): misspelled word\n",
        "        Returns:\n",
        "            list[str]: list of candidates\n",
        "        \"\"\"\n",
        "        return known(edits1(word)) or known(edits2(word))\n",
        "    \n",
        "\n",
        "    def find_grams_freq(self, sequence: list[str], check: bool=False) -> tuple[int, int]:\n",
        "        \"\"\" Method for finding existing n_grams in the corpus that correspond to the current variant.\n",
        "        Args:\n",
        "            sequence (list[str]): list of words: context + possibly correct word\n",
        "        Returns:\n",
        "            : tuple[int, int] where tuple[0] is n in n_gram and \n",
        "                tuple[1] is amount of this n_gram is met in the corpus\n",
        "        Example: \n",
        "            find_grams_freq(['a', 'babe', 'in', 'the',\t'woods']) => \n",
        "            frequencies = [(2, 8805), (3, 630), (4, 16), (5, 16)] that means: \n",
        "                the bigram ('the', 'woods') is met in the vocab 8805 times, the trigram ('in', 'the', 'woods')\n",
        "                is met in the vocab 630 times, etc.\n",
        "            summation = 9467 = 8805+630+16+16\n",
        "            types_of_grams = len(frequencies) that means\n",
        "                in how many types of n_grams (bigrams/trigrams/fourgrams/fivegrams) within the current context\n",
        "                the current possibly correct word is met in the corpus.\n",
        "        \"\"\"\n",
        "        frequences = []\n",
        "        summation = 0\n",
        "\n",
        "        # cut the sequence to 5 words because we have maximum fivegrams\n",
        "        sequence = sequence[-5:]\n",
        "        for i in range(2,len(sequence)+1):\n",
        "            current_context = tuple(sequence[-i:])\n",
        "            if current_context in bigrams.keys():\n",
        "                frequences.append((i, bigrams[current_context]))\n",
        "            elif current_context in threegrams.keys():\n",
        "                frequences.append((i, threegrams[current_context]))\n",
        "            elif current_context in fourgrams.keys():\n",
        "                frequences.append((i, fourgrams[current_context]))\n",
        "            elif current_context in fivegrams.keys():\n",
        "                frequences.append((i, fivegrams[current_context]))\n",
        "\n",
        "        summation = sum([x[1] for x in frequences])\n",
        "        if check:\n",
        "            return frequences, summation\n",
        "        \n",
        "        types_of_grams = len(frequences)\n",
        "        return types_of_grams, summation\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([(2, 8805), (3, 630), (4, 16), (5, 16)], 9467)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrector = Corrector()\n",
        "corrector.find_grams_freq(['a', 'babe', 'in', 'the',\t'woods'], check=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i can t believe how beautiful the sunset was yesterday'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrector.correct_sentence(\"I can't belive how bueatiful the sunset was yesterday.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i red the book yesterday and it was really interesting. i want to tell about it to all my friends'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corrector.correct_text(\"I red the book yesturday and it was really interessing. I wantf \\\n",
        "                       to telk about it to all my frends.\", return_list=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Justification\n",
        "\n",
        "Firstly, I red the paper with the Norvig's solution and highlighted several points: it's based on the frequency in the corpus and considers only grammar typos. His solution doesn't take into account keybord layout and error that might appear because of the incorrect finger position.\n",
        "\n",
        "Secondly, I watched through the provided datasets. There are bigrams.txt, fivegrams.txt, and cocal_all_links.txt with bigrams and corresponding POS tags.\n",
        "\n",
        "I decided to improve the Norvig's solution using these n_grams. While I have fivegrams, I can create threegrams and fourgrams extracting the required amount of words from fivegrams. Doing this, I can achieve better performance, I can get more information about dependencies in the text. However, there is always a problem with lack of data. Although, I have 1m of fivegrams, the language is too flexible to be enclosed in a million of rows. Also, we don't know the position of those fivegrams in sentences. It would give much more information about how texts start and end. I mentioned this, because there is a problem with fixing some typos in the 1st words in sentences. There is no previous context for the 1st words and we rely only on Norvig's correction without n_grams.\n",
        "\n",
        ">I tried to deal with this problem by looking for required POS tags among variants. \n",
        "\n",
        "However, I saw that the 1st words are mostly 'There', 'We', 'I', 'The', 'Me', '“Jane', 'Be', 'It', - short and sometimes bring no meaning (determiners). It is easy to assume that if a short word is misspelled there are too much variants per correction. Thus, this problem might be solved with sentence analysis and more advanced application of POS tags while Norvig's and my approaches give only ~ 45% of accuracy and some similar results:\n",
        "```\n",
        "(Case 3): correct word / corrected word\n",
        "i is\n",
        "they then\n",
        "the the\n",
        "i o\n",
        "the the\n",
        "the the\n",
        "so to\n",
        "each each\n",
        "``` \n",
        "\n",
        ">I improved text annalysis using splitting fivegrams to 3-4-5grams.\n",
        "\n",
        "I assumed that words appearing in a bigger number of various contexts are supposed to be used more frequently. After generating possible corrections for a misspelled word, I consider these options in the context provided. A context+variant pair that appears in the largest number of n_grams and most frequently is chosen. Thus, unlike Norvig’s algorithm, this approach takes into account not only the frequency of occurrence of a word in the corpus as a whole, but also the frequency of its occurrence in the current context.\n",
        "\n",
        "\n",
        "**Testing**\n",
        "\n",
        "For the test part I downloaded the \"Jane Eyre\" book in the same format as \"Sherlock Holmes\" book in the 'big.txt' file. I preprocessed file a little bit and left only the chapters' text: removed the head with titles, authors, edition, preface, etc. Then I processed the file into the string variable and removed some unnecessary titles and punctuation (chapters allignment and several empty lines). As a result, I got a list of clear sentences. Then I synthetically generated mistakes in words using edit1 and edit2 types of typos (with 50% probability per each).\n",
        "\n",
        "I tested algorithms via:\n",
        "1) Spoiling the last words in sentences and fixing them only.\n",
        "2) Spoiling 50% of words and fixing the whole sentence.\n",
        "\n",
        "In the 1st approach I got:\n",
        "```\n",
        "My solution: 0.716\n",
        "Norvig's solution 0.664\n",
        "```\n",
        "To sum up, this approach demonstrates 5.2% of increased accuracy.\n",
        "\n",
        "In the 2nd approach my solution's accuracy is bigger than Norvig's by 1.5%. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test My and Norwigs' solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test set generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['There was no possibility of taking a walk that day',\n",
              " 'We had been wandering, indeed, in the leafless shrubbery an hour in the morning; but since dinner the cold winter wind had brought with it clouds so sombre, and a rain so penetrating, that further outdoor exercise was now out of the question',\n",
              " 'I was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the coming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the chidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to Eliza, John, and Georgiana Reed',\n",
              " 'The said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room: she lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither quarrelling nor crying) looked perfectly happy',\n",
              " 'Me, she had dispensed from joining the group; saying, “She regretted to be under the necessity of keeping me at a distance; but that until she heard from Bessie, and could discover by her own observation, that I was endeavouring in good earnest to acquire a more sociable and childlike disposition, a more attractive and sprightly manner—something lighter, franker, more natural, as it were—she really must exclude me from privileges intended only for contented, happy, little children.” “What does Bessie say I have done?” I asked']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = ''\n",
        "with open('useful data/pg1260.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        test += line\n",
        "\n",
        "test = re.sub(r'\\n\\n\\n\\n\\n\\n', ' ', test)\n",
        "test = re.sub(r'\\n\\n\\n\\n\\n', ' ', test)\n",
        "test = re.sub(r'\\n\\n\\n\\n', ' ', test)\n",
        "test = re.sub(r'\\n\\n\\n', ' ', test)\n",
        "test = re.sub(r'\\n\\n', ' ', test)\n",
        "test = re.sub(r'\\n', ' ', test)\n",
        "test = re.sub(r'\\ufeff', '', test)\n",
        "test = re.sub(r'CHAPTER \\w ', '', test)\n",
        "test = re.sub(r'CHAPTER \\w\\w ', '', test)\n",
        "test = re.sub(r'CHAPTER \\w\\w\\w ', '', test)\n",
        "test = re.sub(r'CHAPTER \\w\\w\\w\\w ', '', test)\n",
        "test = re.sub(r'CHAPTER \\w\\w\\w\\w\\w ', '', test)\n",
        "test = re.sub(r'CHAPTER \\w\\w\\w\\w\\w\\w ', '', test)\n",
        "test = test.split('. ')\n",
        "test[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = test[:500]\n",
        "test_copy = test.copy()[:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There, We, I, The, Me,, “Jane,, Be, It, I, Folds, At, Afar,, I, They, The, I, The, The, So, Each, "
          ]
        }
      ],
      "source": [
        "for i in range(20):\n",
        "    print(test[i].split()[0], end=', ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Case 1: errors in the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================================\n",
        "# Case 1:\n",
        "# the only mistake is in the last word\n",
        "# =================================================================\n",
        "\n",
        "import random\n",
        "random.seed(1)\n",
        "\n",
        "def make_mistake(words: list[str], return_list: bool=True, position: int=-1):\n",
        "    last_word = words[position]\n",
        "    # choose the mistakes type\n",
        "    choice = random.randint(0, 1)\n",
        "    if choice == 0:\n",
        "        mistake_words = edits1(last_word)\n",
        "    else:\n",
        "        mistake_words = edits2(last_word)\n",
        "    # randomly select one incorrect word and replace it\n",
        "    incorrect_word = random.choice(list(mistake_words))\n",
        "    words[position] = incorrect_word\n",
        "\n",
        "    # return the required type\n",
        "    if return_list: return words\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My solution: 0.716\n",
            "Norvig's solution 0.664\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# CHECK MY SOLUTION\n",
        "# =================================================================\n",
        "\n",
        "count = 0\n",
        "total = 0\n",
        "corrector = Corrector()\n",
        "for sentence in test:\n",
        "    # iteratively go through the test set\n",
        "    if sentence == '':\n",
        "        continue\n",
        "    total += 1\n",
        "    # get separate words from a sentence and extract the last one\n",
        "    words = get_words(sentence)\n",
        "    correct_word = words[-1]\n",
        "    # add mistakes and return the string\n",
        "    incorrect_sentence = make_mistake(words, return_list=False)\n",
        "    # fix mistakes\n",
        "    corrected_word = corrector.correct_sentence(incorrect_sentence).split()[-1]\n",
        "    # compare\n",
        "    if corrected_word == correct_word:\n",
        "        count += 1\n",
        "my_result = count / (total)\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# CHECK NORVIG'S SOLUTION\n",
        "# =================================================================\n",
        "\n",
        "count = 0\n",
        "total = 0\n",
        "for sentence in test:\n",
        "    if sentence == '':\n",
        "        continue\n",
        "    total += 1\n",
        "    # get separate words and the last word\n",
        "    words = get_words(sentence)\n",
        "    correct_word = words[-1]\n",
        "    # add a mistake to the last word\n",
        "    incorrect_sentence = make_mistake(words)\n",
        "    # correct mistakes in the last word using Norvig's solution\n",
        "    corrected_word = correction(incorrect_sentence[-1])\n",
        "    if corrected_word == correct_word:\n",
        "        count += 1\n",
        "norvig_result = count / (total)\n",
        "\n",
        "\n",
        "print('My solution:', my_result)\n",
        "print(\"Norvig's solution\", norvig_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Case 2: 50% words have errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================================\n",
        "# Case 2:\n",
        "# The same data from Jane Eyre book\n",
        "# mistakes are among the sentences, about 50% of words are incorrect\n",
        "# =================================================================\n",
        "\n",
        "def make_several_mistakes(words: list[str], return_list: bool=True):\n",
        "    incorrect = words.copy()\n",
        "    # misspell 50% of words\n",
        "    for i,word in enumerate(incorrect):\n",
        "        if random.randint(0, 1) == 0:\n",
        "            # then misspell\n",
        "            # type of mistake:\n",
        "            choice = random.randint(0, 1)\n",
        "            if choice == 0:\n",
        "                mistake_words = edits1(word)\n",
        "            else:\n",
        "                mistake_words = edits2(word)\n",
        "            incorrect_word = random.choice(list(mistake_words))\n",
        "            incorrect[i] = incorrect_word\n",
        "\n",
        "    if return_list: return incorrect\n",
        "    return ' '.join(incorrect)\n",
        "\n",
        "\n",
        "def apply_norvig_correction(words: list[str]):\n",
        "    # method for Norvig's correction of the whole sentence\n",
        "    corrected = []\n",
        "    for word in words:\n",
        "        if word in WORDS:\n",
        "            corrected.append(word)\n",
        "            continue\n",
        "        corrected.append(correction(word))\n",
        "    return corrected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My solution: 0.8231830156516349\n",
            "Norvig's solution: 0.8135750813575081\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# CHECK MY SOLUTION\n",
        "# =================================================================\n",
        "random.seed(10)\n",
        "\n",
        "count = 0\n",
        "total = 0\n",
        "corrector = Corrector()\n",
        "for sentence in test_copy:\n",
        "    if sentence == '':\n",
        "        continue\n",
        "    words = get_words(sentence)\n",
        "    incorrect_sentence = make_several_mistakes(words, return_list=False)\n",
        "    corrected_sentence = corrector.correct_sentence(incorrect_sentence).split()\n",
        "\n",
        "    for correct, corrected in zip(words, corrected_sentence):\n",
        "        total += 1\n",
        "        if correct == corrected:\n",
        "            count += 1\n",
        "my_result = count / total\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# CHECK NORVIG'S SOLUTION\n",
        "# =================================================================\n",
        "count = 0\n",
        "total = 0\n",
        "for sentence in test_copy:\n",
        "    if sentence == '':\n",
        "        continue\n",
        "    words = get_words(sentence)\n",
        "    correct_sentence = ' '.join(words)\n",
        "    incorrect_sentence = make_several_mistakes(words)\n",
        "    corrected_sentence = apply_norvig_correction(incorrect_sentence)\n",
        "\n",
        "    for correct, corrected in zip(words, corrected_sentence):\n",
        "        total += 1\n",
        "        if correct == corrected:\n",
        "            count += 1\n",
        "norvig_result = count/total\n",
        "\n",
        "\n",
        "print('My solution:', my_result)\n",
        "print(\"Norvig's solution:\", norvig_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Case 3: errors in the 1st words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My solution: 0.438\n",
            "Norvig's solution 0.484\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# Case 3: 1st words have error\n",
        "# the only mistake is in the first word\n",
        "# =================================================================\n",
        "\n",
        "random.seed(10)\n",
        "\n",
        "# =================================================================\n",
        "# CHECK MY SOLUTION\n",
        "# =================================================================\n",
        "count = 0\n",
        "total = 0\n",
        "corrector = Corrector()\n",
        "for sentence in test:\n",
        "    # iteratively go through the test set\n",
        "    if sentence == '':\n",
        "        continue\n",
        "    total += 1\n",
        "    # get separate words from a sentence and extract the last one\n",
        "    words = get_words(sentence)\n",
        "    correct_word = words[0]\n",
        "    # add mistakes and return the string\n",
        "    incorrect_sentence = make_mistake(words, return_list=False, position=0)\n",
        "    # fix mistakes\n",
        "    corrected_word = corrector.correct_sentence(incorrect_sentence).split()[0]\n",
        "    # compare\n",
        "    if corrected_word == correct_word:\n",
        "        count += 1\n",
        "my_result = count / total\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# CHECK NORVIG'S SOLUTION\n",
        "# =================================================================\n",
        "\n",
        "count = 0\n",
        "total = 0\n",
        "for sentence in test:\n",
        "    if sentence == '':\n",
        "        continue\n",
        "    total += 1\n",
        "    # get separate words and the last word\n",
        "    words = get_words(sentence)\n",
        "    correct_word = words[0]\n",
        "    # add a mistake to the last word\n",
        "    incorrect_sentence = make_mistake(words, position=0)\n",
        "    # correct mistakes in the last word using Norvig's solution\n",
        "    corrected_word = correction(incorrect_sentence[0])\n",
        "    if corrected_word == correct_word:\n",
        "        count += 1\n",
        "norvig_result = count / total\n",
        "\n",
        "\n",
        "print('My solution:', my_result)\n",
        "print(\"Norvig's solution\", norvig_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
